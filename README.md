![Databricks](https://img.shields.io/badge/Databricks-Data%20Engineering-red)
![Delta Lake](https://img.shields.io/badge/Delta-Lake-blue)
![Power BI](https://img.shields.io/badge/PowerBI-Dashboard-yellow)

# ğŸš— Plateforme Data Insurance â€“ Architecture Medallion & Dashboard BI

## ğŸ“Œ PrÃ©sentation du projet

Ce projet met en Å“uvre une architecture Medallion (Bronze â†’ Silver â†’ Gold) sur Databricks Lakeflow (Declarative Pipelines), combinÃ©e Ã  un Data Warehouse en modÃ¨le Ã©toile et un dashboard Power BI stratÃ©gique.

Lâ€™objectif est de simuler une plateforme dâ€™analyse assurance automobile production-ready, intÃ©grant :

- Bonnes pratiques Data Engineering
- ModÃ©lisation dimensionnelle (Star Schema)
- Delta Lake
- Data Quality Monitoring
- Mesures DAX avancÃ©es
- Analyse risque mÃ©tier

## ğŸ—ï¸ Architecture

### ğŸ”· Architecture Medallion

Raw Data (CSV)
     â†“
Bronze (Streaming - Autoloader)
     â†“
Silver (Nettoyage + QualitÃ© + Feature Engineering)
     â†“
Gold
   â”œâ”€â”€ Data Warehouse (ModÃ¨le Ã©toile)
   â””â”€â”€ Data Marts analytiques

## ğŸ¥ DÃ©monstration de lâ€™exÃ©cution du pipeline

La vidÃ©o suivante montre lâ€™orchestration complÃ¨te du pipeline Lakeflow 

(Bronze â†’ Silver â†’ Warehouse â†’ Marts) ainsi que la matÃ©rialisation des tables Delta.

Elle illustre :
- Les dÃ©pendances entre tables
- Lâ€™ordre dâ€™exÃ©cution
- La matÃ©rialisation des couches
- Le monitoring des runs

ğŸ‘‰![DAG Execution Demo](screenshots/dag_execution.gif)

### ğŸ¥‰ Bronze Layer

- Ingestion streaming via Databricks Autoloader

- DonnÃ©es brutes stockÃ©es en Delta

- Ã‰volution de schÃ©ma activÃ©e

### ğŸ¥ˆ Silver Layer

**Transformations rÃ©alisÃ©es** :

- Cast des types (Ã¢ge, statut sinistre, NCAP)

- Normalisation boolÃ©enne

- Parsing des colonnes moteur (torque / power via regex)

- Ajout dâ€™une date technique dâ€™ingestion

- Suppression des doublons

- Data Quality Expectations intÃ©grÃ©es

**RÃ¨gles de qualitÃ©** :

- Ã‚ge client â‰¥ 18

- Ã‚ge vÃ©hicule â‰¥ 0

- Claim status âˆˆ {0,1}

### ğŸ¥‡ Gold Layer

ğŸ”¹ **Data Warehouse (ModÃ¨le en Ã©toile)**

Table de faits :

- wh_fact_claims

Tables de dimensions :

- wh_dim_region
- wh_dim_segment
- wh_dim_fuel
- wh_dim_ncap
- wh_dim_date

Ce modÃ¨le permet :

- AgrÃ©gations dynamiques cÃ´tÃ© BI
- Analyse multi-axes
- Performance optimisÃ©e

ğŸ”¹ **Data Marts**

Tables agrÃ©gÃ©es :

- mart_claim_by_fuel
- mart_claim_by_segment
- mart_claim_metrics
- mart_risk_by_ncap
- mart_data_quality_summary

--- 

## ğŸ—‚ï¸ ModÃ¨le en Ã©toile

![ModÃ¨le en Ã©toile](screenshots/star_schema_model.png)

---

## ğŸ“Š Dashboard Power BI


### ğŸŸ¦ 1. Executive Overview

![Executive Overview](screenshots/01_executive_overview.png)

### ğŸŸ¦ 2. Risk Analytics

**Analyse approfondie du risque** :

![Risk Analytics](screenshots/02_risk_analytics.png)

### ğŸŸ¦ 3. Temporal Analytics

**Analyse temporelle avancÃ©e** :

![Temporal Analytics](screenshots/03_temporal_analytics.png)

ğŸŸ¦ 4. Data Quality Monitoring

![Data Quality](screenshots/04_data_quality.png)

## ğŸ“ Mesures DAX avancÃ©es

Exemples :

Claim Frequency =
DIVIDE(
    SUM(wh_fact_claims[claim_status]),
    COUNT(wh_fact_claims[policy_id])
)


Relative Risk Index =
DIVIDE(
    [Claim Frequency],
    CALCULATE([Claim Frequency], ALL(wh_dim_segment))
)

---

## âš™ï¸ Optimisations & Industrialisation

**Format Delta Lake** :
Les donnÃ©es sont stockÃ©es au format Delta afin de garantir la fiabilitÃ© transactionnelle (ACID), lâ€™optimisation des performances et la gestion simplifiÃ©e des Ã©volutions de schÃ©ma.

**Lakeflow Declarative Pipeline** :
Les transformations sont orchestrÃ©es via Lakeflow, permettant une gestion dÃ©clarative, reproductible et industrialisable du pipeline de donnÃ©es.

**Expectations qualitÃ© intÃ©grÃ©es** :
Des rÃ¨gles de validation (Ã¢ge client, Ã¢ge vÃ©hicule, statut sinistre) sont appliquÃ©es dÃ¨s la couche Silver afin dâ€™assurer la fiabilitÃ© analytique en aval.

**Star Schema BI-ready** :
La couche Gold est structurÃ©e en modÃ¨le Ã©toile (fact + dimensions) pour garantir des performances optimales et une exploitation flexible dans Power BI.

**Pipeline planifiÃ© (schedule)** :
Le pipeline est exÃ©cutÃ© automatiquement via un planificateur, simulant un environnement de production avec rafraÃ®chissement rÃ©gulier des donnÃ©es.

**Optimisation automatique Delta** :
Les mÃ©canismes dâ€™auto-optimisation (optimize write, auto compaction) sont activÃ©s afin dâ€™amÃ©liorer les performances de lecture et rÃ©duire la fragmentation.

**Simulation temporelle rÃ©aliste** :
Une distribution temporelle des donnÃ©es sur plusieurs annÃ©es a Ã©tÃ© simulÃ©e pour permettre des analyses YTD, YoY et tendances mensuelles crÃ©dibles.

---

ğŸ§  CompÃ©tences dÃ©montrÃ©es

**Data Engineering (Databricks)** :
Conception et orchestration dâ€™un pipeline complet de traitement de donnÃ©es sur une plateforme cloud moderne.

**Architecture Medallion** :
ImplÃ©mentation structurÃ©e Bronze â†’ Silver â†’ Gold garantissant sÃ©paration des responsabilitÃ©s et qualitÃ© progressive des donnÃ©es.

**ModÃ©lisation dimensionnelle** :
CrÃ©ation dâ€™un modÃ¨le en Ã©toile optimisÃ© pour lâ€™analyse dÃ©cisionnelle et la performance BI.

**Delta Lake** :
Utilisation dâ€™un format transactionnel robuste permettant la fiabilitÃ©, lâ€™optimisation et lâ€™Ã©volutivitÃ© des tables analytiques.

Data Quality Monitoring :
Mise en place dâ€™indicateurs de qualitÃ© et de suivi volumÃ©trique pour sÃ©curiser la chaÃ®ne analytique.

**DAX avancÃ©** :
CrÃ©ation de mesures dynamiques (Time Intelligence, Index de risque, segmentation) pour produire des indicateurs stratÃ©giques.

**Storytelling BI** :
Structuration du dashboard en pages orientÃ©es dÃ©cision (Executive, Risk, Temporal, Monitoring).

**Optimisation analytique** :
Conception orientÃ©e performance en sÃ©parant Data Warehouse et Data Marts.

---

ğŸ› ï¸ Stack technique

Databricks Lakeflow :
Orchestration dÃ©clarative du pipeline et gestion automatisÃ©e des dÃ©pendances.

Delta Lake :
Stockage transactionnel optimisÃ© pour analytics.

Unity Catalog :
Gestion centralisÃ©e de la gouvernance et des objets analytiques.

PySpark :
Transformation distribuÃ©e et feature engineering.

SQL :
RequÃªtes analytiques et validation des donnÃ©es.

Power BI :
Visualisation stratÃ©gique et crÃ©ation dâ€™indicateurs dÃ©cisionnels.

DAX :
ModÃ©lisation analytique avancÃ©e et calculs dynamiques.

## ğŸš€ AmÃ©liorations futures

**SCD Type 2** :
ImplÃ©mentation de dimensions historisÃ©es afin de gÃ©rer lâ€™Ã©volution des attributs mÃ©tier dans le temps.

**Historisation des runs** :
Ajout dâ€™une table de suivi des exÃ©cutions pour monitorer les performances et volumes traitÃ©s.

**Monitoring avancÃ©** :
Mise en place dâ€™alertes et dâ€™indicateurs techniques pour simuler un environnement production.

**CI/CD** :
IntÃ©gration Git et automatisation des dÃ©ploiements.

**Liquid Clustering** :
Optimisation avancÃ©e des performances pour les tables volumineuses.

**Tests automatisÃ©s** :
Ajout de tests unitaires et contrÃ´les de cohÃ©rence sur les transformations.

## ğŸ“‚ Structure du repository

/src
   bronze_claims.py
   silver_claims.py
   wh_fact_claims.py
   wh_dim_*.py
   mart_*.py

/screenshots
   executive_overview.png
   risk_analytics.png
   temporal_analytics.png
   data_quality.png
   star_schema_model.png

README.md






